/*
 * Implementation of Error Back-Propagation Learning Algorithm
 *      Programmed by Yoo Jang-Hee (in AI Lab. SERI, at KIST)
 *                 DEC. 5, 1990  Written in  C Language
 */

# include <stdio.h>
# include <math.h>
/*
# include "menu.h"
*/

/* define constants used throughout functions */

# define MXN_INAT    8   /* max no. of input features   */
# define MXN_HDLR    1   /* max no. of hidden layers    */
# define MXN_UNIT   16   /* max no. of units in a layer */
# define MXN_OUAT    2   /* max no. of output features  */
# define MXN_SAMP  800   /* max no. of input samples    */

/* Database : declarations of variables */

float InPut[MXN_SAMP][MXN_INAT];        /* input pattern storage */
float Target[MXN_SAMP][MXN_OUAT];       /* target output storage */

float activation[MXN_HDLR+1][MXN_UNIT];    /* activation storage */   

float weight[MXN_HDLR+1][MXN_UNIT][MXN_INAT];  /* current weight */
float dweight[MXN_HDLR+1][MXN_UNIT][MXN_INAT];   /* delta weight */

float bias[MXN_HDLR+1][MXN_UNIT];                /* current bias */
float dbias[MXN_HDLR+1][MXN_UNIT];                 /* delta bias */
float error[MXN_HDLR+1][MXN_UNIT];              /* current error */
float delta[MXN_HDLR+1][MXN_UNIT];                /* delta error */

float lrate = 0.5, momentum = 0.9; /* learning rate and momentum */
float cur_err, pss, tss;        /* current error and total error */

int  epochno, no_input, no_layer, no_unit[MXN_HDLR+2];
char pt_name[MXN_SAMP][12];

char *mnuMain[]	= {
    "Learning",
    "Recognition",
    "Exit", 
NULL };
char *hiddenNode[] = {
     "4", "5", "6", "7", "8",
     "9", "10", "11", "12"
};
char *epochData[] = {
     "30", "40", "50", "60", "70"
};
enum { LEARN, RECOG, EXIT};

main()
{
    char task_name[12];
    int  crow = 6, ccol = 15, choice;
    int  i, j, k = 0;

/*
    _clearscreen(_GCLEARSCREEN);
    _settextcolor(2); _settextposition(20, 5);
    _outtext("BP> Enter the task name : ");
*/
    printf("BP> Enter the task name : ");
    scanf("%s", task_name);

    param_define(task_name);
    define_bp_network();
    read_input_pattern(task_name);
    
    for(j=0; j<5; j++) {
        for (i=0; i<9; i++) {
            printf("recognition order : hidden = %d, epoch = %d\n",
                    no_unit[1], epochno);
            recognition(task_name);
            no_unit[1] += 1;
        }
        epochno += 1000;
    } 
}

int menu(items)
char *items[];
{
     int  i;

     for(i=0; i<3; i++) {
        printf("\t%s\n", items[i]);
     }
     while(1) {
        i = getc(stdin);
        if(i=='l' || i=='L')
            return('l');
        else if(i=='r' || i=='R')
            return('r');
        else if(i=='q' || i=='Q')
            return('q');
     }
}

learning()
/*
 * learning procedure
 */
{   
    int   i, epoch = 0;
    float max_pss = 0.01, cur_tss = 100.0;

    /* _clearscreen(_GCLEARSCREEN); */
/*
    _settextposition( 5, 27); _outtext("-.Epochno : ");
    _settextposition( 6, 27); _outtext("-.Pattern : ");
    _settextposition( 7, 27); _outtext("-.PSS     : ");
    _settextposition( 8, 27); _outtext("-.TSS     : ");
*/
    while ((epoch < epochno) && (max_pss > pss) && (cur_tss > tss)) {
        cur_tss = 0.0;
        for (i = 0; i < no_input; i++) {      /* each pattern learning */
            compute_output(i); /* output compute by current weight */
            compute_error(i);     /* error compute by target value */
            change_weight(i);           /* weight adjust of output */
            if (cur_err > max_pss) max_pss = cur_err;
            cur_tss = cur_tss + cur_err;
/*
            _settextposition( 6, 39); printf(" %s (%d)\t", pt_name[i], i+1);
*/
        }
/*
        if ((epoch % 1) == 0) {
            _settextposition( 5, 39); printf(" %d\t\t", epoch + 1);
            _settextposition( 7, 39); printf(" %f\t", cur_err);
            _settextposition( 8, 39); printf(" %f\t", cur_tss);
        }
*/
        epoch++;
    }
    printf("Epoch =  %d\t\t", epoch + 1);
    printf("PSS   =  %f\t", cur_err);
    printf("TSS   =  %f\t", cur_tss);
}

recognition(task_name)
char task_name[];
/*
 * recognition
 */
{   
    FILE *fpo;
    int  i, j, tag, count, scount;
    float percent, percent1;
    char fname[20], ans[10];
    
    /* If task is already in the memorey, data files for task do not 
       need to be read in.  But, if it is a new task, data files should
       read in to reconstruct the net */
/*
    printf("\n\n\n\n\n\n Generation of outputs for a new pattern");
    printf("\n\t Work on a different task ?  ");
    printf("\n\t   Answer (y/n) : ");
    scanf("%s", ans);
*/
    ans[0] = 'y';
    if ((ans[0] == 'y') || (ans[0] == 'Y')) {
        read_neural_weight(task_name);
    }

/*
    printf("\n\n\n\n\n\n Outputs File Creation (y/n) : ");
    scanf("%s", ans);
*/
    count = 0;
    scount = 0;
    ans[0] = 'y';
    if ((ans[0] == 'y') || (ans[0] == 'Y')) {
        strcpy(fname, task_name);
        strcat(fname, hiddenNode[no_unit[1]-4]);
        strcat(fname, epochData[epochno/1000 - 3]);
        strcat(fname, ".out");
        fpo = fopen(fname, "w");
        for (i = 0; i < no_input; i++) {
            compute_output(i);
            fprintf(fpo, " Name : %s ==>", pt_name[i]);
            fprintf(fpo, " Target=( ");
            for (j = 0; j < no_unit[no_layer-1]; j++)
                fprintf(fpo, "%1.0f ", Target[i][j]);
            fprintf(fpo, ")  Actual=[ ");
            for (j = 0; j < no_unit[no_layer-1]; j++)
                fprintf(fpo, "%8.6f ", activation[1][j]);

	    for (j = 0; j < no_unit[no_layer-1]; j++) {
		if (activation[1][j] > 0.5) activation[1][j] = 1.0;
		else activation[1][j] = 0.0;
	    }
	    tag = 0;
	    for (j = 0; j < no_unit[no_layer-1]; j++) {
		if ((j == 0) && (Target[i][j] == 1)) scount++;
		if ((Target[i][j] - activation[1][j]) != 0.0) tag = 1;
	    }
	    fprintf(fpo, "]");
	    if (tag == 0) {
	      fprintf(fpo, "  yes\n");
	      count++;
	    }
	    else  fprintf(fpo, "  no !!!!!\n");
	}
	percent = ((float)count/(float)no_input) * 100.;
	fprintf(fpo, "input pattern count = %d\n", no_input);
	fprintf(fpo, "correct recognition pattern count = %d\n", count);
	fprintf(fpo, "percent = %f\n\n", percent);

	count = no_input - count;
	count = scount - count;
	percent1 = ((float)count/(float)scount) * 100.;
	fprintf(fpo, "input Swing pattern = %d\n", scount);
	fprintf(fpo, "Police percent = %f\n", percent1);
	fclose(fpo);
    }
/*
    _clearscreen(_GCLEARSCREEN);
*/
}

/*--------------------------------------------------------*
 *  Parameter, Input Pattern and Weight File I/O Routine  *
 *--------------------------------------------------------*/

param_define(task_name)
char task_name[];
/*
 * network define parameter read
 */
{   
    no_layer = 3;
    no_unit[0] = 8;
    no_unit[1] = 4;
    no_unit[2] = 2;
    no_input   = 600;
    lrate = 0.1;
    momentum = 0.9;
    epochno = 3000;
    pss = 0.001;
    tss = 0.1;
}

read_input_pattern(task_name)
char task_name[];
/* 
 * read input pattern from data file 
 */
{
    FILE *fpl;
    int  i, j, k;
    char fname[20];
   
    strcpy(fname, task_name);
    strcat(fname, ".dat");
    if ((fpl = fopen(fname, "r")) == NULL) {
	printf("Can't Open Input Data File %s\n", fname);
        exit(0);
    }

    i = 0;
    for (j = 0; j < no_input; j++) {
        fscanf(fpl, "%s", pt_name[j]);
        /* printf("\n%s", pt_name[j]); */
        for (k = 0; k < no_unit[i]; k++) {
            fscanf(fpl, "%f", &InPut[j][k]);
            /* printf(" %6.5f", InPut[j][k]); */
        }
        for (k = 0; k < no_unit[no_layer-1]; k++) {
            fscanf(fpl, "%f", &Target[j][k]);
            /* printf(" %1.0f", Target[j][k]); */
        }
    }
    fclose(fpl);
}

write_neural_weight(task_name)
char task_name[];
/*
 * learning weight write */
{
    FILE *fpw;
    int  i, j, k;
    char fname[20];

    strcpy(fname, task_name);
    strcat(fname, hiddenNode[no_unit[1]-4]);
    strcat(fname, epochData[epochno/1000 - 3]);
    strcat(fname, ".weg");
    fpw = fopen(fname, "w");
    for (i = 0; i < (no_layer-1); i++) {
        for (j = 0; j < no_unit[i+1]; j++) {
            for (k = 0; k < no_unit[i]; k++)
                fprintf(fpw, "%f\n", weight[i][j][k]);
            fprintf(fpw, "%f\n", bias[i][j]);
        }
    }
    fprintf(fpw, "%d\n", epochno);
    fprintf(fpw, "%d\n", no_unit[1]);
    fclose(fpw);
}

read_neural_weight(task_name)
char task_name[];
/*
 * read  learning weight
 */
{
    FILE *fpr;
    int  i, j, k;
    char fname[20];

    strcpy(fname, task_name);
    strcat(fname, hiddenNode[no_unit[1]-4]);
    strcat(fname, epochData[epochno/1000 - 3]);
    strcat(fname, ".weg");
    if ((fpr = fopen(fname, "r")) == NULL) {
	printf("Can't Open Learning Weight File %s\n", fname);
        exit(0);
    }
    for (i = 0; i < (no_layer-1); i++) {
        for (j = 0; j < no_unit[i+1]; j++) {
            for (k = 0; k < no_unit[i]; k++)
                fscanf(fpr, "%f", &weight[i][j][k]);
            fscanf(fpr, "%f", &bias[i][j]);
        }
    }
    fscanf(fpr, "%d", epochno);
}

/*--------------------------------------------------------*
 *           Back-Propagation Learning Routine            *
 *--------------------------------------------------------*/

float random() 
/*
 * random number generator
 */
{
    return((float)rand()*3.051850948e-5);
}

float sigmoid(x)
float x;
/*
 * logistic activation function
 */
{
    double exp();

    if (x > 11.5129) return(0.99999);
    else
        if (x < -11.5129) return(0.00001);
    return(1.0 / (1.0 + (float) exp( (double) ((-1.0) * x))));
}

define_bp_network()
/*
 * network define and random number assignment 
 */
{
    int  i, j, k;
    long time;   
 
    srand(&time);
    for (i = 0; i < (no_layer-1); i++) {
        for (j = 0; j < no_unit[i+1]; j++) {
            for (k = 0; k < no_unit[i]; k++)
                weight[i][j][k] = random();
            bias[i][j] = random();
        }
    }
}

compute_output(pe)
int pe;
/*
 * net input, hidden data summation and net output generation
 */
{
    int i, j, k;
    float sum_of_input;
    
    for (i = 0; i < (no_layer-1); i++) {
        for (j = 0; j < no_unit[i+1]; j++) {
            sum_of_input = bias[i][j];
            for (k = 0; k < no_unit[i]; k++) { /* input summation */
                if (i == 0)
                    sum_of_input += InPut[pe][k] * weight[i][j][k];
                else
                    sum_of_input += activation[i-1][k] * weight[i][j][k];
            }
            activation[i][j] = sigmoid(sum_of_input);
        }
    }
}

compute_error(l)
int l;
/*
 * error compute between output value and taget value
 */
{
    int i, j, k;
    
    i = 0;
    for (j = 0; j < no_unit[i+1]; j++)
        error[i][j] = 0.0; /* initialize */
    
    i = 1;
    cur_err = 0.0;
    for (j = 0; j < no_unit[i+1]; j++) { /* error between target and output */
        error[i][j] = Target[l][j] - activation[i][j]; /* delta value computing by GDR */
        delta[i][j] = error[i][j] * activation[i][j] * (1.0 - activation[i][j]);
        cur_err = cur_err + fabs(error[i][j]);
        for (k = 0; k < no_unit[i]; k++)/* error computing for hidden layer */
            error[i-1][k] += delta[i][j] * weight[i][j][k];
    }

    i = 0;
    for (j = 0; j < no_unit[i+1]; j++)
        delta[i][j] = error[i][j] * activation[i][j] * (1.0 - activation[i][j]);
}



change_weight(pe)
int pe;
{
    int i, j, k;
    
    for (i = 0; i < (no_layer-1); i++) {
        for (j = 0; j < no_unit[i+1]; j++) {
            for (k = 0; k < no_unit[i]; k++) {
                /* compute to change value of connection link */
                if (i == 0)
                    dweight[i][j][k] = lrate * delta[i][j] * InPut[pe][k] + momentum * dweight[i][j][k];
                else
                    dweight[i][j][k] = lrate * delta[i][j] * activation[i-1][k] + momentum * dweight[i][j][k];
                weight[i][j][k] += dweight[i][j][k];
            }
            /* same method to adjust the bias value */
            dbias[i][j] = lrate * delta[i][j] + momentum * dbias[i][j];
            bias[i][j] += dbias[i][j];
        }
    }
}

/*-------------------------- End of Program ----------------------------*/

