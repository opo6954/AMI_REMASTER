/*
 * Implementation of Error Back-Propagation Learning Algorithm
 *      Programmed by Yoo Jang-Hee (in AI Lab. SERI, at KIST)
 *                 DEC. 5, 1990  Written in  C Language
 */

# include <stdio.h>
# include <math.h>
# include <string.h>
/*
# include "menu.h"
*/

/* define constants used throughout functions */

#define MXN_INAT    32   /* max no. of input features   */
#define MXN_HDLR    1   /* max no. of hidden layers    */
#define MXN_UNIT    16   /* max no. of units in a layer */
#define MXN_OUAT    4   /* max no. of output features  */
#define MXN_SAMP    10000   /* max no. of input samples    */

/* Database : declarations of variables */

float InPut[MXN_SAMP][MXN_INAT];        /* input pattern storage */
float Target[MXN_SAMP][MXN_OUAT];       /* target output storage */

float activation[MXN_HDLR+1][MXN_UNIT];    /* activation storage */   

float weight[MXN_HDLR+1][MXN_UNIT][MXN_INAT];  /* current weight */
float dweight[MXN_HDLR+1][MXN_UNIT][MXN_INAT];   /* delta weight */

float bias[MXN_HDLR+1][MXN_UNIT];                /* current bias */
float dbias[MXN_HDLR+1][MXN_UNIT];                 /* delta bias */
float error[MXN_HDLR+1][MXN_UNIT];              /* current error */
float delta[MXN_HDLR+1][MXN_UNIT];                /* delta error */

double lrate = 0.2, momentum = 0.8; /* learning rate and momentum */
double cur_err, pss, tss;        /* current error and total error */

int  epochno, no_input, no_layer, no_unit[MXN_HDLR+2];
char pt_name[MXN_SAMP][50];



/*--------------------------------------------------------*
 *  Parameter, Input Pattern and Weight File I/O Routine  *
 *--------------------------------------------------------*/

void param_define(task_name)
char task_name[];
/*
 * network define parameter read
 */
{   
    no_layer = 3;
    no_unit[0] = MXN_INAT;
    no_unit[1] = MXN_UNIT;
    no_unit[2] = MXN_OUAT;
    no_input   = MXN_SAMP;
    lrate = 0.2;
    momentum = 0.8;
    epochno = 10000;
    pss = 0.001;
    tss = 0.1;
	
}

void read_input_pattern(task_name)
char task_name[];
/* 
 * read input pattern from data file 
 */
{
    FILE *fpl;
    int  i, j, k;
    char fname[20];
   
    strcpy(fname, task_name);
    strcat(fname, ".dat");
    if ((fpl = fopen(fname, "r")) == NULL) {
		printf("Can't Open Input Data File %s\n", fname);
        exit(-1);
    }
    
	fscanf(fpl, "%d", &no_input);

    i = 0;
    for (j = 0; j < no_input; j++) {
        fscanf(fpl, "%s", pt_name[j]);
        /* printf("\n%s", pt_name[j]); */
        for (k = 0; k < no_unit[i]; k++) {
            fscanf(fpl, "%f", &InPut[j][k]);
            /* printf(" %6.5f", InPut[j][k]); */
        }
        for (k = 0; k < no_unit[no_layer-1]; k++) {
            fscanf(fpl, "%f", &Target[j][k]);
            /* printf(" %1.0f", Target[j][k]); */
        }
    }
    fclose(fpl);
}


void read_neural_weight(task_name)
char task_name[];
/*
 * read  learning weight
 */
{
    FILE *fpr;
    int  i, j, k;
    char fname[40] = "";

    strcpy(fname, task_name);
	strcat(fname, ".weg");
    //strcat(fname, "train.weg");
    if ((fpr = fopen(fname, "r")) == NULL) {
	    printf("Can't Open Learning Weight File %s\n", fname);
        exit(-1);
    }
	
    for (i = 0; i < (no_layer-1); i++) {
        for (j = 0; j < no_unit[i+1]; j++) {
            for (k = 0; k < no_unit[i]; k++) {
                fscanf(fpr, "%f", &weight[i][j][k]);
				//printf("%f\n", weight[i][j][k]);
			}
            fscanf(fpr, "%f", &bias[i][j]);
			//printf("%f\m", bias[i][j]);
        }
    }
	
    fscanf(fpr, "%d", &epochno);
	fclose(fpr);
	
}


double sigmoid(x)
float x;
/*
 * logistic activation function
 */
{
    double exp();

    if (x > 11.5129) return(0.99999);
    else
        if (x < -11.5129) return(0.00001);
    return(1.0 / (1.0 + (float) exp( (double) ((-1.0) * x))));
}



void compute_output(pe)
int pe;
/*
 * net input, hidden data summation and net output generation
 */
{
    int i, j, k;
    float sum_of_input;
    
    for (i = 0; i < (no_layer-1); i++) {
        for (j = 0; j < no_unit[i+1]; j++) {
            sum_of_input = bias[i][j];
            for (k = 0; k < no_unit[i]; k++) { /* input summation */
                if (i == 0)
                    sum_of_input += InPut[pe][k] * weight[i][j][k];
                else
                    sum_of_input += activation[i-1][k] * weight[i][j][k];
            }
            activation[i][j] = (float)sigmoid(sum_of_input);
        }
    }
}

void compute_error(l)
int l;
/*
 * error compute between output value and taget value
 */
{
    int i, j, k;
    
    i = 0;
    for (j = 0; j < no_unit[i+1]; j++)
        error[i][j] = 0.0; /* initialize */
    
    i = 1;
    cur_err = 0.0;
    for (j = 0; j < no_unit[i+1]; j++) { /* error between target and output */
        error[i][j] = Target[l][j] - activation[i][j]; /* delta value computing by GDR */
        delta[i][j] = error[i][j] * activation[i][j] * ((float)1.0 - activation[i][j]);
        cur_err = cur_err + fabs(error[i][j]);
        for (k = 0; k < no_unit[i]; k++)/* error computing for hidden layer */
            error[i-1][k] += delta[i][j] * weight[i][j][k];
    }

    i = 0;
    for (j = 0; j < no_unit[i+1]; j++)
        delta[i][j] = error[i][j] * activation[i][j] * ((float)1.0 - activation[i][j]);
}


void recognition(task_name)
char task_name[];
/*
 * recognition
 */
{   
    FILE *fpo;
    int  i, j, tag, count = 0;
    float percent;
    char fname[100];
        
	
	read_neural_weight(task_name);
    
    strcpy(fname, task_name);
    strcat(fname, ".out");
    fpo = fopen(fname, "w");
    for (i = 0; i < no_input; i++) {
        compute_output(i);
        fprintf(fpo, " Name : %s ==>", pt_name[i]);
        fprintf(fpo, " Target=( ");
        for (j = 0; j < no_unit[no_layer-1]; j++)
            fprintf(fpo, "%1.0f ", Target[i][j]);
        fprintf(fpo, ")  Actual=[ ");
        for (j = 0; j < no_unit[no_layer-1]; j++)
            fprintf(fpo, "%8.6f ", activation[1][j]);

	    for (j = 0; j < no_unit[no_layer-1]; j++) {
			if (activation[1][j] > 0.5) activation[1][j] = 1.0;
			else activation[1][j] = 0.0;
	    }
	    tag = 0;
	    for (j = 0; j < no_unit[no_layer-1]; j++) {		
			if ((Target[i][j] - activation[1][j]) != 0.0) tag = 1;
	    }

	    fprintf(fpo, "]");
	    if (tag == 0) {
			fprintf(fpo, "  yes\n");
			count++;
	    }
	    else  fprintf(fpo, "  no !!!!!\n");
	}
	percent = ((float)count/(float)no_input) * (float)100.;
	fprintf(fpo, "input pattern count = %d\n", no_input);
	fprintf(fpo, "correct recognition pattern count = %d\n", count);
	fprintf(fpo, "percent = %f\n\n", percent);
	
	fclose(fpo);  

}

main()
{
    char task_name[100];

    printf("BP> Enter the task name : ");
    scanf("%s", task_name);

    param_define(task_name);
    read_input_pattern(task_name);

	printf("recognition\n");
    recognition(task_name);
     
}



/*-------------------------- End of Program ----------------------------*/

